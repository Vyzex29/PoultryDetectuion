{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pytorch/yolo install\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: Too many arguments.\n",
      "\n",
      "usage: git clone [<options>] [--] <repo> [<dir>]\n",
      "\n",
      "    -v, --verbose         be more verbose\n",
      "    -q, --quiet           be more quiet\n",
      "    --progress            force progress reporting\n",
      "    --reject-shallow      don't clone shallow repository\n",
      "    -n, --no-checkout     don't create a checkout\n",
      "    --bare                create a bare repository\n",
      "    --mirror              create a mirror repository (implies bare)\n",
      "    -l, --local           to clone from a local repository\n",
      "    --no-hardlinks        don't use local hardlinks, always copy\n",
      "    -s, --shared          setup as shared repository\n",
      "    --recurse-submodules[=<pathspec>]\n",
      "                          initialize submodules in the clone\n",
      "    --recursive ...       alias of --recurse-submodules\n",
      "    -j, --jobs <n>        number of submodules cloned in parallel\n",
      "    --template <template-directory>\n",
      "                          directory from which templates will be used\n",
      "    --reference <repo>    reference repository\n",
      "    --reference-if-able <repo>\n",
      "                          reference repository\n",
      "    --dissociate          use --reference only while cloning\n",
      "    -o, --origin <name>   use <name> instead of 'origin' to track upstream\n",
      "    -b, --branch <branch>\n",
      "                          checkout <branch> instead of the remote's HEAD\n",
      "    -u, --upload-pack <path>\n",
      "                          path to git-upload-pack on the remote\n",
      "    --depth <depth>       create a shallow clone of that depth\n",
      "    --shallow-since <time>\n",
      "                          create a shallow clone since a specific time\n",
      "    --shallow-exclude <revision>\n",
      "                          deepen history of shallow clone, excluding rev\n",
      "    --single-branch       clone only one branch, HEAD or --branch\n",
      "    --no-tags             don't clone any tags, and make later fetches not to follow them\n",
      "    --shallow-submodules  any cloned submodules will be shallow\n",
      "    --separate-git-dir <gitdir>\n",
      "                          separate git dir from working tree\n",
      "    -c, --config <key=value>\n",
      "                          set config inside the new repository\n",
      "    --server-option <server-specific>\n",
      "                          option to transmit\n",
      "    -4, --ipv4            use IPv4 addresses only\n",
      "    -6, --ipv6            use IPv6 addresses only\n",
      "    --filter <args>       object filtering\n",
      "    --remote-submodules   any cloned submodules will use their remote-tracking branch\n",
      "    --sparse              initialize sparse-checkout file to include only files at root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "#!cd yolov5\n",
    "#!pip install -r requirements.txt  # install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import torch, load model\n",
    "yolo versions - https://pytorch.org/hub/ultralytics_yolov5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to C:\\Users\\User\\AppData\\Roaming\\Ultralytics\\Arial.ttf...\n",
      "YOLOv5  2022-4-8 torch 1.11.0 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f426e028a734bbaa467602ca89cbc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/14.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plot\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webcam test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # yolo can do a video, which is cool, just passing instead of 0 a videofile is good\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    result = model(frame)\n",
    "    cv2.imshow('YOLO', np.squeeze(result.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord ('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  YOLO Labeling images from collected images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating with Barack (1).jpg\n",
      "Created imageXml at:  C:\\Users\\User\\PycharmProjects\\tensorFlowObjectDetection\\TFRecognition\\Tensorflow\\workspace\\images\\collectedimages\\Barack\\Barack (1).xml\n",
      "Created txt at:  C:\\Users\\User\\PycharmProjects\\tensorFlowObjectDetection\\TFRecognition\\YOLO\\labels\\Barack (1).txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPicturePipeline\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mPicturePipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunPicturePipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\tensorFlowObjectDetection\\PicturePipeline.py:217\u001b[0m, in \u001b[0;36mrunPicturePipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(file, final_image_array)\n\u001b[1;32m--> 217\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# Q exit\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import PicturePipeline\n",
    "\n",
    "PicturePipeline.runPicturePipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Creating dataset.yaml\n",
    "Train yaml tutorial : https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: ../TFRecognition/YOLO\n",
      "train: images\n",
      "val: images\n",
      "\n",
      "nc: 10\n",
      "names: ['Barack', 'Elon', 'Eminem', 'Hasan', 'Hillary', 'Iggy', 'Kendrick', 'Lisa', 'Rihanna', 'Valera']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import Configuration.Config as config\n",
    "import TensorTrain as tt\n",
    "\n",
    "YOLO5_PATH = os.path.join(config.BASE_DIR, 'yolov5')\n",
    "dataset_file = os.path.join(YOLO5_PATH, 'dataset.yaml')\n",
    "labels = tt.GetLabels()\n",
    "file_name = \"classes.txt\"\n",
    "YOLO_LABELS_PATH = config.yolo['SAVE_LABELS_PATH']\n",
    "YOLO_LABELS_CLASSES = YOLO_LABELS_FILE = os.path.join(YOLO_LABELS_PATH, file_name)\n",
    "result = tt.file_to_list(YOLO_LABELS_CLASSES)\n",
    "\n",
    "yamlString = f\"path: {'../TFRecognition/YOLO'}\\n\" \\\n",
    "             f\"train: images\\n\" \\\n",
    "             f\"val: images\\n\" \\\n",
    "             f\"\\n\" \\\n",
    "             f\"nc: {len(labels)}\\n\" \\\n",
    "             f\"names: [\"\n",
    "\n",
    "for label in result:\n",
    "    yamlString += f\"'{label}', \"\n",
    "\n",
    "size = len(yamlString)\n",
    "mod_string = yamlString[:size - 2]\n",
    "mod_string += \"]\"\n",
    "print(mod_string)\n",
    "with open(dataset_file, \"w\") as f:\n",
    "    f.write(mod_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#   Training Yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# maybe try workers 2, with rtx3070 it seems a bit buggy\n",
    "!cd yolov5 && python train.py --img 200 --batch 16 --epochs 5000 --data dataset.yaml --workers 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#   Launching yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-5-23 Python-3.9.7 torch-1.11.0 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7037095 parameters, 0 gradients, 15.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "# force_reload = true if cache is not responding\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='TFRecognition/YOLO/100img5000epochs200x200/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # yolo can do a video, which is cool, just passing instead of 0 a videofile is good\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    result = model(frame)\n",
    "    cv2.imshow('YOLO', np.squeeze(result.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord ('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press q key to continue\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "path = \"C:/Users/User/PycharmProjects/tensorFlowObjectDetection/testImages\"\n",
    "for root, dirs, dirfiles in os.walk(path):\n",
    "    for file in dirfiles:\n",
    "        if file.endswith(\"png\") or file.endswith(\"jpg\") or file.endswith(\"JPG\") or file.endswith(\"jpeg\"):\n",
    "            imagePath = os.path.join(root, file)\n",
    "            img = cv2.imread(imagePath)\n",
    "            image_np = np.array(img)\n",
    "            result = model(image_np)\n",
    "            cv2.imshow('YOLO', np.squeeze(result.render()))\n",
    "            print(\"Press q key to continue\")\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            # Q exit\n",
    "            if key == ord(\"q\"):\n",
    "                cv2.destroyAllWindows()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
