{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pytorch/yolov7 install\n",
    "\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "https://github.com/WongKinYiu/yolov7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!pip install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "# add a requirements_gpu.txt where\n",
    "#!git clone https://github.com/WongKinYiu/yolov7  # clone\n",
    "!cd yolov7\n",
    "!pip install -r requirements.txt  # install"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import torch, load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-5-31 Python-3.9.16 torch-1.13.1 CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5m.pt to yolov5m.pt...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04183320c6b34e46b881bfecab6b9ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/40.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plot\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webcam test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # yolo can do a video, which is cool, just passing instead of 0 a videofile is good\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    result = model(frame)\n",
    "    cv2.imshow('YOLO', np.squeeze(result.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord ('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  YOLO Labeling images from collected images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating with Barack (1).jpg\n",
      "Created imageXml at:  C:\\Users\\User\\PycharmProjects\\tensorFlowObjectDetection\\TFRecognition\\Tensorflow\\workspace\\images\\collectedimages\\Barack\\Barack (1).xml\n",
      "Created txt at:  C:\\Users\\User\\PycharmProjects\\tensorFlowObjectDetection\\TFRecognition\\YOLO\\labels\\Barack (1).txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mPicturePipeline\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mPicturePipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunPicturePipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\tensorFlowObjectDetection\\PicturePipeline.py:217\u001B[0m, in \u001B[0;36mrunPicturePipeline\u001B[1;34m()\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     cv2\u001B[38;5;241m.\u001B[39mimshow(file, final_image_array)\n\u001B[1;32m--> 217\u001B[0m     key \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwaitKey\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;241m&\u001B[39m \u001B[38;5;241m0xFF\u001B[39m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;66;03m# Q exit\u001B[39;00m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mord\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mq\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import PicturePipeline\n",
    "\n",
    "PicturePipeline.runPicturePipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Creating dataset.yaml\n",
    "Train yaml tutorial : https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: ../TFRecognition/YOLO\n",
      "train: images\n",
      "val: images\n",
      "\n",
      "nc: 10\n",
      "names: ['Barack', 'Elon', 'Eminem', 'Hasan', 'Hillary', 'Iggy', 'Kendrick', 'Lisa', 'Rihanna', 'Valera']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import Configuration.Config as config\n",
    "import TensorTrain as tt\n",
    "\n",
    "YOLO5_PATH = os.path.join(config.BASE_DIR, 'yolov5')\n",
    "dataset_file = os.path.join(YOLO5_PATH, 'dataset.yaml')\n",
    "labels = tt.GetLabels()\n",
    "file_name = \"classes.txt\"\n",
    "YOLO_LABELS_PATH = config.yolo['SAVE_LABELS_PATH']\n",
    "YOLO_LABELS_CLASSES = YOLO_LABELS_FILE = os.path.join(YOLO_LABELS_PATH, file_name)\n",
    "result = tt.file_to_list(YOLO_LABELS_CLASSES)\n",
    "\n",
    "yamlString = f\"path: {'../TFRecognition/YOLO'}\\n\" \\\n",
    "             f\"train: images\\n\" \\\n",
    "             f\"val: images\\n\" \\\n",
    "             f\"\\n\" \\\n",
    "             f\"nc: {len(labels)}\\n\" \\\n",
    "             f\"names: [\"\n",
    "\n",
    "for label in result:\n",
    "    yamlString += f\"'{label}', \"\n",
    "\n",
    "size = len(yamlString)\n",
    "mod_string = yamlString[:size - 2]\n",
    "mod_string += \"]\"\n",
    "print(mod_string)\n",
    "with open(dataset_file, \"w\") as f:\n",
    "    f.write(mod_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#   Training Yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# maybe try workers 2, with rtx3070 it seems a bit buggy\n",
    "!cd yolov5_custom && python train.py --img 640 --batch 16 --epochs 100 --data custom_data.yaml --workers 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#   Launching yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-72-g064365d Python-3.9.16 torch-1.13.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "# force_reload = true if cache is not responding\n",
    "\n",
    "#model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5bird.pt')\n",
    "model = torch.hub.load('yolov5', 'custom', path='yolov5bird.pt', source='local') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"chicken.mp4\") # yolo can do a video, which is cool, just passing instead of 0 a videofile is good\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    result = model(frame)\n",
    "    cv2.imshow('YOLO', np.squeeze(result.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord ('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press q key to continue\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "path = \"C:/Users/User/PycharmProjects/tensorFlowObjectDetection/testImages\"\n",
    "for root, dirs, dirfiles in os.walk(path):\n",
    "    for file in dirfiles:\n",
    "        if file.endswith(\"png\") or file.endswith(\"jpg\") or file.endswith(\"JPG\") or file.endswith(\"jpeg\"):\n",
    "            imagePath = os.path.join(root, file)\n",
    "            img = cv2.imread(imagePath)\n",
    "            image_np = np.array(img)\n",
    "            result = model(image_np)\n",
    "            cv2.imshow('YOLO', np.squeeze(result.render()))\n",
    "            print(\"Press q key to continue\")\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            # Q exit\n",
    "            if key == ord(\"q\"):\n",
    "                cv2.destroyAllWindows()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}